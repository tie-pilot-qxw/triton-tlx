#ifndef NV_DIALECT_HOPPER_TRANSFORMS_CODEPARTITIONUTILITY_H_
#define NV_DIALECT_HOPPER_TRANSFORMS_CODEPARTITIONUTILITY_H_

#include "triton/Dialect/Triton/IR/Dialect.h"
#include "triton/Dialect/TritonGPU/IR/Dialect.h"
#include "triton/Dialect/TritonGPU/Transforms/Utility.h"
#include "triton/Dialect/TritonNvidiaGPU/IR/Dialect.h"

#include "Utility.h"
#include <algorithm>
#include <numeric>

namespace mlir {

namespace tt = mlir::triton;

enum class DataChannelKind : int {
  SMEM = 0,
  TMEM = 1,
  REG = 2,
  SMEMPost = 3,
  TMEMPost = 4
};

static inline std::string to_string(DataChannelKind k) {
  switch (k) {
  case DataChannelKind::SMEM:
    return "smem";
  case DataChannelKind::TMEM:
    return "tmem";
  case DataChannelKind::REG:
    return "reg";
  case DataChannelKind::SMEMPost:
    return "smem_post";
  case DataChannelKind::TMEMPost:
    return "tmem_post";
  }
  return "Unknown";
}

struct Channel {
public:
  using Relation = std::pair<int, SmallVector<int>>;

  Channel(int producer, SmallVector<int> &consumers, Operation *op,
          unsigned operandIdx, unsigned numBuffers, unsigned ID,
          DataChannelKind channelKind = DataChannelKind::SMEM)
      : relation(producer, consumers), op(op), operandIdx(operandIdx),
        _numBuffers(numBuffers), uniqID(ID), channelKind(channelKind) {}

  bool operator==(const Channel &c) {
    return relation == c.relation && operandIdx == c.operandIdx && op == c.op;
  }
  virtual ~Channel() = default;

  virtual Operation *getDstOp() { return op; }
  unsigned getDstOperandIdx() { return operandIdx; }
  Value getSrcOperand() { return op->getOperand(operandIdx); }
  virtual Operation *getSrcOp() { return getSrcOperand().getDefiningOp(); }
  virtual Operation *getAllocOp() { return nullptr; }
  virtual unsigned getNumBuffers() { return _numBuffers; }
  virtual Operation *getDstOpLast() { return nullptr; }
  virtual void getDstOps(SmallVector<Operation *> &dsts) {}

  Relation relation; // producer task Id, a list of consumer task Ids
  Operation *op;
  unsigned operandIdx;
  unsigned _numBuffers;
  DataChannelKind channelKind = DataChannelKind::SMEM;
  unsigned uniqID;
};

// A few assumptions, a channel can have multiple consumers, but the consumers
// must be in the same region and the taskIds must be the same. We can have
// a representative consumer in the channel.
struct ChannelPost : Channel {
public:
  using Relation = std::pair<int, SmallVector<int>>;

  // source can be local_store, consumer can be gen5, ttg.memdesc_trans,
  // local_load
  ChannelPost(int producer, SmallVector<int> &consumers, Operation *allocOp,
              unsigned ID)
      : Channel(producer, consumers, nullptr, 0 /*operandIdx*/, 0, ID),
        allocOp(allocOp) {
    channelKind = DataChannelKind::SMEMPost;
  }

  bool operator==(const ChannelPost &c) {
    return relation == c.relation && allocOp == c.allocOp;
  }
  virtual ~ChannelPost() = default;

  virtual Operation *getSrcOp();
  virtual Operation *getDstOp();
  virtual Operation *getDstOpLast();
  virtual void getDstOps(SmallVector<Operation *> &dsts);
  virtual Operation *getAllocOp() { return allocOp; }
  virtual unsigned getNumBuffers();

  Operation *allocOp;
};

struct ReuseGroup {
  std::vector<unsigned> channelIDs;
  std::vector<Channel *> channels;
};

struct ReuseConfig {
  // Each ReuseGroup
  std::vector<ReuseGroup> groups;
  unsigned getGroupSize() { return groups.size(); }
  ReuseGroup *getGroup(unsigned idx) {
    assert(idx < groups.size());
    return &groups[idx];
  }
};

struct CommChannel {
  DenseMap<int, Value> tokens;
  // Producer barrier is only needed when the producer op itself can update the
  // barrier inline, such as the TMA load.
  std::optional<Value> producerBarrier;
  // Consumer barrier is only needed when the consumer op itself can update the
  // barrier inline, such as the TCGen5MMAOp.
  DenseMap<int, Value> consumerBarriers;
};

namespace ttng = ::mlir::triton::nvidia_gpu;
namespace triton {
namespace nvidia_gpu {
struct TmemDataChannel : Channel {
  ttng::TMEMAllocOp tmemAllocOp;
  ttng::TCGen5MMAOp tmemMmaOp;
  Operation *tmemProducerOp;

  TmemDataChannel(int producer, SmallVector<int> &consumers,
                  ttng::TMEMAllocOp tmemAllocOp, ttng::TCGen5MMAOp tmemMmaOp,
                  Operation *tmemLoadOp, unsigned operandIdx,
                  unsigned numBuffers, unsigned uniqID)
      : Channel(producer, consumers, tmemLoadOp, operandIdx, numBuffers,
                uniqID),
        tmemAllocOp(tmemAllocOp), tmemProducerOp(tmemAllocOp),
        tmemMmaOp(tmemMmaOp) {
    assert(consumers.size() == 1 &&
           "TmemDataChannel must have a single consumer");
    channelKind = DataChannelKind::TMEM;
  }

  ttng::TMEMAllocOp getTmemAllocOp() { return tmemAllocOp; }
  virtual Operation *getAllocOp() { return nullptr; }
  ttng::TCGen5MMAOp getMmaOp() { return tmemMmaOp; }
  virtual Operation *getSrcOp() { return tmemProducerOp; }
};

struct TmemDataChannelPost : Channel {
  bool isOperandD;
  bool isOperandDNoAcc;
  Operation *allocOp;

  // Can be produced by tmem_store or operand D of gen5, consumed by tmem_load
  // or gen5
  TmemDataChannelPost(int producer, SmallVector<int> &consumers,
                      Operation *allocOp, bool isOperandD, bool isOperandDNoAcc,
                      unsigned uniqID)
      : Channel(producer, consumers, nullptr, 0 /*operandIdx*/, 0, uniqID),
        isOperandD(isOperandD), isOperandDNoAcc(isOperandDNoAcc),
        allocOp(allocOp) {
    channelKind = DataChannelKind::TMEMPost;
  }

  virtual Operation *getSrcOp();
  virtual Operation *getDstOp();
  virtual unsigned getNumBuffers();
  virtual Operation *getAllocOp() { return allocOp; }
  virtual Operation *getDstOpLast();
  virtual void getDstOps(SmallVector<Operation *> &dsts);
};
} // namespace nvidia_gpu
} // namespace triton

bool enclosing(scf::IfOp ifOp, Operation *op);
bool enclosing(scf::ForOp forOp, Operation *op);

// Return number of AccumCnts for the given ctrlOp. AccumCnts due to reuses
// will be at the end, we go through all ReuseGroups and if any channel in
// the group is nested under ctrlOp, we add one accumCnt for this group.
unsigned getAccumCnts(Operation *ctrlOp,
                      const DenseSet<Operation *> &regionsWithChannels,
                      ReuseConfig *config);

// We pass in groupIdx, if it is -1, we are getting accumCnt for a channel
// not in a reuse group, directly in ctrlOp. ctrlOp can be null if
// reuseGroupIdx >= 0.
unsigned getAccumArgIdx(scf::ForOp parentForOp, Operation *ctrlOp,
                        const DenseSet<Operation *> &regionsWithChannels,
                        ReuseConfig *config, int reuseGroupIdx);

void getReuseChannels(ReuseGroup *gruop, Operation *regionOp,
                      SmallVector<Operation *> &chList);
// Skip the accumCnt for unique channels.
unsigned getReuseAccumArgIdx(Operation *regionOp,
                             const DenseSet<Operation *> &regionsWithChannels,
                             ReuseConfig *config, int reuseGroupIdx);

SmallVector<Operation *>
getTaskTopRegion(triton::FuncOp funcOp, const SmallVector<Channel *> &channels);

void appendAccumCntsForOps(SmallVector<Operation *> &taskTopOps,
                           const SmallVector<Channel *> &channels,
                           DenseSet<Operation *> &regionsWithChannels,
                           ReuseConfig *config);

void collectRegionsWithChannels(const SmallVector<Channel *> &channels,
                                DenseSet<Operation *> &regionsWithChannels);
void collectRegionsWithChannelsPost(const SmallVector<Channel *> &channels,
                                    DenseSet<Operation *> &regionsWithChannels);
void insertAsyncCopy(
    triton::FuncOp funcOp,
    const DenseMap<Channel *, SmallVector<Channel *>>
        &channelsGroupedByProducers,
    const DenseMap<Channel *, Value> &bufferMap,
    DenseMap<Channel *, std::pair<Operation *, Operation *>> &copyOpMap,
    DenseSet<Operation *> &regionsWithChannels, ReuseConfig *config,
    bool isPost = false);

Value getAccumCount(OpBuilderWithAsyncTaskIds &builder, Operation *op,
                    const DenseSet<Operation *> &regionsWithChannels,
                    ReuseConfig *config, int reuseGroupIdx);
std::pair<Value, Value> getBufferIdxAndPhase(OpBuilderWithAsyncTaskIds &builder,
                                             Location loc, Value accumCnt,
                                             unsigned numBuffers);
void getBufferIdxAndPhase(OpBuilderWithAsyncTaskIds &builder, Operation *op,
                          unsigned numBuffers,
                          const DenseSet<Operation *> &regionsWithChannels,
                          Value &bufferIdx, Value &phase, ReuseConfig *config,
                          int reuseGroupIdx = -1);

Value getBarrierForPipelineStage(OpBuilderWithAsyncTaskIds &builder,
                                 Value barrierAlloc, Value bufferIdx);

Operation *optimizeTMALoads(OpBuilderWithAsyncTaskIds &builder,
                            SmallVector<tt::DescriptorLoadOp> &tmaLoads,
                            SmallVector<Value> &buffers, Value barrierAlloc,
                            Value bufferIdx, Value bufferIdxExtract,
                            Value phase, Operation *headProducer,
                            Operation *headConsumer,
                            Operation *headConsumerSameLevel,
                            bool isPost = false);
void specializeRegion(triton::FuncOp funcOp, unsigned requestedRegisters);
Value createBufferView(OpBuilderWithAsyncTaskIds &builder, Value alloc,
                       Value idx);
void collectPostChannels(SmallVector<std::unique_ptr<Channel>> &channels,
                         triton::FuncOp &funcOp);

Operation *getSameLevelOp(Operation *p, Operation *c);
SmallVector<Operation *> getActualConsumers(Operation *consumerOp);
} // namespace mlir

#endif // NV_DIALECT_HOPPER_TRANSFORMS_CODEPARTITIONUTILITY_H_
